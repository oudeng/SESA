# SESA: Missing Data Imputation Based on Structural Equation Modeling Enhanced with Self-Attention

## What's SESA?
An innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of Self-Attention neural networks. 
![FOSA Framework](Fig/FOSA_framework.png)

## Why SESA?
Our comprehensive experiments on both simulated and real-world datasets underscore SESAâ€™s pronounced advantages over traditional baseline techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Especially on small and middle size dataset.

## SESA paper (Under Review)
In the folder **/paper**, or see it at arXiv: [https://arxiv.org/submit/5075308
](https://arxiv.org/abs/2308.12388)
## SESA code (Updated code coming soon. At present, the old version of FOSA_v2.)
In the folder **/code**.   
- ***We advocate for the use of FOSA_v2, a distinguished utility designed for the imputation of missing data within any CSV data file. This recommendation is further bolstered by the provision of an illustrative Python code sample, which underscores its accessibility and clarity.***
- FOSA_v1 elucidates our advanced proposition combining the FIML methodology with a Self-Attention mechanism. Rigorous evaluations of its functionality and efficiency have been undertaken on both simulated datasets and an authentic health dataset.
