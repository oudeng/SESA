# SESA: Missing Data Imputation Based on Structural Equation Modeling Enhanced with Self-Attention

## What's SESA?
An innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of Self-Attention neural networks. 
![SESA Mechanism](Fig/Fig_SESA.png)

## Why SESA?
Our comprehensive experiments on both simulated and real-world datasets underscore SESAâ€™s pronounced advantages over traditional baseline techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Especially on small and middle size dataset.
![SESA Mechanism](Fig/Fig_Exp.png)

## SESA paper (Under Review)
In the folder **/paper**, or see it at arXiv: [https://arxiv.org/submit/5075308
](https://arxiv.org/abs/2308.12388)
## SESA code (Updated code coming soon. At present, the old version of SESA is FOSA_v2.)
